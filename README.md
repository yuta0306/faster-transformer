# faster-transformer

*Self-attention Does Not Need $O(n^2)$ Memory*のPytorch実装

```bibtex
@misc{rabe2021selfattention,
    title={Self-attention Does Not Need $O(n^2)$ Memory}, 
    author={Markus N. Rabe and Charles Staats},
    year={2021},
    eprint={2112.05682},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
```
